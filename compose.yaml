# Load environment variables from .env file
# Make sure to set GEMINI_API_KEY in your .env file

services:
  agents:
    image: hacksy/agents
    build:
      context: agent
    ports:
      - "7777:7777"
    environment:
      # point agents at the MCP gateway
      - MCPGATEWAY_URL=mcp-gateway:8811
      # Gemini API configuration
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      # GitHub API configuration
      - GITHUB_PERSONAL_ACCESS_TOKEN=${GITHUB_PERSONAL_ACCESS_TOKEN}
    volumes:
      # mount the agents configuration
      - ./agents.yaml:/agents.yaml
    depends_on:
      - mcp-gateway
    networks:
      - ai-network
    restart: unless-stopped

  
  agents-ui:
    image: hacksy/ui
    build:
      context: agent-ui
    ports:
      - "3003:3003"
    environment:
      - NODE_ENV=development
      - NEXT_TELEMETRY_DISABLED=1
      - PORT=3003
      - AGENTS_URL=http://agents:7777
    depends_on:
      - agents
    networks:
      - ai-network
    restart: unless-stopped

  mcp-gateway:
    # mcp-gateway secures your MCP servers
    image: docker/mcp-gateway:latest
    # use docker API socket to start MCP servers
    use_api_socket: true
    command:
      # securely embed secrets into the gateway
      - --secrets=/run/secrets/mcp_secret
      # add any MCP servers you want to use
      - --servers=github-official,duckduckgo,fetch
      # add an interceptor to format GitHub user data
      - --interceptor
      - after:exec:cat | jq '.content[0].text = (.content[0].text | fromjson | map(select(. != null) | [(.login // ""), (.name // ""), (.public_repos // ""), (.followers // ""), (.following // ""), (.bio // "")] | @csv) | join("\n"))'
    secrets:
      - mcp_secret
    volumes:
       - /var/run/docker.sock:/var/run/docker.sock
       - ./data:/app/data:ro
    networks:
       - ai-network
    restart: unless-stopped


# Define available AI models
models:
  qwen3-small:
    # Lightweight model for development
    model: ai/qwen3:8B-Q4_0 # 4.44 GB
    context_size: 15000 # 7 GB VRAM
  
  qwen3-medium:
    # More capable model for production
    model: ai/qwen3:14B-Q6_K # 11.28 GB
    context_size: 15000 # 15 GB VRAM
  
  qwen2-5:
    # Alternative model option
    model: qwen2.5:latest
    context_size: 8192

# Mount the secrets file for MCP servers
secrets:
  mcp_secret:
    file: ./.mcp.env

networks:
  ai-network:
     driver: bridge

volumes:
  model-cache:
  agent-data:
